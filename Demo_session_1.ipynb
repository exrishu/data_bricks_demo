{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a64ddd1-381e-4db5-888d-5ee435f5438d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Demo Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9ff2e0-2fd2-4cd0-82ba-293482751ed5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebooks have some Apache Spark variables already defined.\n",
    "# SparkContext: sc\n",
    "# SQLContext/HiveContext: sqlContext\n",
    "# SparkSession (Spark 2.x): spark\n",
    "\n",
    "print(\"Spark version\", sc.version, spark.sparkContext.version, spark.version)\n",
    "print(\"Python version\", sc.pythonVer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8cafa6-2a37-4133-9928-b29e9c5245d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "println(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d03a65-db80-440d-99ce-b541019980b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"https://timeseries.surge.sh/usd_to_eur.csv\")\n",
    "df = spark.read.csv(sc.parallelize(r.text.splitlines()), header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a840803f-6743-4008-b5b4-0122e336040b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = dbutils.widgets.get(\"container\")\n",
    "dbutils.jobs.taskValues.set(key = 'container', value = container)\n",
    "storageAccount = dbutils.widgets.get(\"storageAccount\")\n",
    "accessKey = dbutils.widgets.get(\"accessKey\")\n",
    "# container = \"demo-container\"\n",
    "# storageAccount = \"demoaccountstora\"\n",
    "# accessKey = \"ydMAq8qniBp3afwrRK+qxzejhdEFu1KdmFT0UnggaPLbcDVHsl5a4PVcW+6Np4T1JMAm1BR+tQKY+AStBJuwRg==\"\n",
    "\n",
    "accountKey = \"fs.azure.account.key.{}.blob.core.windows.net\".format(storageAccount)\n",
    "\n",
    "# Set the credentials to Spark configuration\n",
    "spark.conf.set(\n",
    "  accountKey,\n",
    "  accessKey)\n",
    "\n",
    "# Set the access key also in SparkContext to be able to access blob in RDD\n",
    "# Hadoop configuration options set using spark.conf.set(...) are not accessible via SparkContext..\n",
    "# This means that while they are visible to the DataFrame and Dataset API, they are not visible to the RDD API.\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\n",
    "  accountKey,\n",
    "  accessKey)\n",
    "\n",
    "# Mount the drive for native python\n",
    "inputSource = \"wasbs://{}@{}.blob.core.windows.net\".format(container, storageAccount)\n",
    "mountPoint = \"/mnt/\" + container\n",
    "extraConfig = {accountKey: accessKey}\n",
    "\n",
    "print(\"Mounting: {}\".format(mountPoint))\n",
    "\n",
    "try:\n",
    "  dbutils.fs.mount(\n",
    "    source = inputSource,\n",
    "    mount_point = str(mountPoint),\n",
    "    extra_configs = extraConfig\n",
    "  )\n",
    "  print(\"=> Succeeded\")\n",
    "except Exception as e:\n",
    "  if \"Directory already mounted\" in str(e):\n",
    "    print(\"=> Directory {} already mounted\".format(mountPoint))\n",
    "  else:\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba11632-281b-4cf9-ae7c-d9d63c23a18b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1689e7f-8a7b-47db-9507-82ac42aa2920",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5572667d-b652-431d-8b20-fa70233767cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using a WASB file path formatted like this:\n",
    "# wasbs://<containername>@<accountname>.blob.core.windows.net/<partialPath>\n",
    "# WASB (Windows Azure Storage Blob) is an extension built on top of the HDFS APIs. HDFS, the Hadoop Distributed File System, is one of the core Hadoop components that manage data and storage on multiple nodes.\n",
    "inputFilePath = \"wasbs://{}@{}.blob.core.windows.net/{}\".format(container, storageAccount, \"/demo_data.csv\")\n",
    "df = spark.read.format(\"csv\").load(inputFilePath, header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1707b905-b281-41b6-97a8-24d29a506c51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Using a mount point on worker nodes with Databricks FS protocol and request files using a file path like:\n",
    "# dbfs:/mnt/<containername>/<partialPath>\n",
    "inputFilePath = \"dbfs:/mnt/{}/{}\".format(container, \"demo_data.csv\")\n",
    "df = spark.read.format(\"csv\").load(inputFilePath, header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c23552f-13b4-4b2b-8b3a-dce0ed8344d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8836940-914d-46ea-8afd-41e999e8caba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df4b260-c606-4eab-847d-97bed590d6ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a91e94e-6455-46a9-b7f3-6d974193c4a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can register the input dataframe as a temporary view named xrate in the SQL context\n",
    "df.createOrReplaceTempView(\"erate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb811a3-1210-41a7-a3c1-f193f35bf130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "e_df=spark.sql(\"select YEAR(Date) as year, COUNT(Date) as count, Mean(Rate) as mean \\\n",
    "    from erate \\\n",
    "        GROUP BY YEAR(Date) order by year DESC\")\n",
    "display(e_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2b30ae-644a-4d17-9c32-6b42ac1b3d8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "retDF = (\n",
    "  df\n",
    "  .groupBy(f.year(\"Date\").alias(\"year\"))\n",
    "  .agg(f.count(\"Date\").alias(\"count\"), f.mean(\"Rate\").alias(\"mean\"))\n",
    "  .sort(f.desc(\"year\"))\n",
    ")\n",
    "\n",
    "display(retDF.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd104ff-83dd-4396-b5df-d495d05e7390",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "plots = []\n",
    "pandaData = df.toPandas().reset_index().set_index('Date')\n",
    "hd_trace = go.Scatter(x=pandaData.index, y=pandaData[\"Rate\"], name=\"Rate\")\n",
    "plots.append(hd_trace)\n",
    " \n",
    "# Plot  \n",
    "p = py.plot(plots, output_type='div')\n",
    "\n",
    "displayHTML(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeec8bc-9efa-4dd5-805a-2f132ec7185b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(\"Date\", \"Rate\").where(\"Date = '2000-01-01'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8140c2-b0b8-45e1-a5ed-9ed4e9a4287a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_with_year = df.withColumn(\"year\", year(df[\"Date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7263f89-4077-4e0a-8d12-a04dd987aa64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd7acc6-c8d4-451c-8ba3-68f3be845216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = f\"wasbs://{container}@{storageAccount}.blob.core.windows.net/demo_data_process.csv\"\n",
    "df_with_year.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo_session_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
